{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import urllib.request as libreq\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from objects import Entry\n",
    "from utils import create_q,downloader, check_col_downloaded_value\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for the arXiv API\n",
    "url = 'http://export.arxiv.org/api/query?search_query='\n",
    "\n",
    "# Initialize an empty list to store the downloaded papers\n",
    "papers = []\n",
    "\n",
    "# Get the current working directory\n",
    "os_path = pathlib.Path().resolve()\n",
    "\n",
    "# Create a unique save path for the downloaded papers based on the current date and time\n",
    "save_path = 'Papers-%s' % datetime.today().strftime('%Y-%m-%d %H-%M-%S')\n",
    "\n",
    "# Combine the working directory and save path to create a full path\n",
    "full_path = os_path / save_path\n",
    "\n",
    "# Create the directory for the downloaded papers if it doesn't already exist\n",
    "Path(str(full_path)).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query terms and options for the arXiv API search\n",
    "qs = [('Interpolation',), [('Interpolation',)],['cs.CV'], ('KDD OR ICML OR NeurIPS OR ICLR OR COLT OR CIKM OR AAAI OR UAI OR IJCAI OR VLDB OR AISTATS',), ]\n",
    "options = ['abs', 'ti', 'cat', 'co']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://export.arxiv.org/api/query?search_query=(abs:Interpolation)+OR+(ti:Interpolation)+AND+(cat:cs.CV)+AND+(co:KDD+OR+ICML+OR+NeurIPS+OR+ICLR+OR+COLT+OR+CIKM+OR+AAAI+OR+UAI+OR+IJCAI+OR+VLDB+OR+AISTATS)&start=0&max_results=339&sortBy=lastUpdatedDate&sortOrder=descending\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89ec77055c14fc28ab77a25ea7a74c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/4.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e284b3473d5b48e19c2094ac39ac3809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4579299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████████████████████▊   | 22/23 [00:10<00:00,  2.08it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3ec34fd72f40fabd5b51edde0d1723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/8.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1775e39f19465fa9b75c334e87edeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8021165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    }
   ],
   "source": [
    "# Initialize a log dictionary to track information\n",
    "log = {}\n",
    "\n",
    "# Define the main function for fetching and processing papers\n",
    "def main(url, download_path, start=1, max=100):\n",
    "    # Read the existing CSV file into a DataFrame\n",
    "    df = pd.read_csv('downloaded.csv')\n",
    "\n",
    "    # Build the query string and options\n",
    "    q = create_q(qs, options).replace(' ', '+')\n",
    "    q_options = f'&start=0&max_results={max}&sortBy=lastUpdatedDate&sortOrder=descending'\n",
    "    print(f'{url}{q[:-4]}{q_options}')\n",
    "\n",
    "    # Fetch data from the specified URL\n",
    "    with libreq.urlopen(f'{url}{q[:-4]}{q_options}') as url:\n",
    "        r = url.read()\n",
    "\n",
    "    # Parse HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(r, 'lxml')\n",
    "\n",
    "    # Extract entries from the HTML content\n",
    "    entries = soup.find_all('entry')\n",
    "\n",
    "    # Iterate over entries and process each one\n",
    "    for entry in tqdm(entries, leave=False):\n",
    "        downloaded = False\n",
    "        e = Entry(entry)\n",
    "        e.scrape()\n",
    "        data = e.get_info()\n",
    "        comp_cat = '_'.join(data['category'])\n",
    "        ven_pup = data['comment']\n",
    "        title = data['title'].translate(str.maketrans('', '', string.punctuation)).replace('\\n', '').replace(' ', '-') + '.pdf'\n",
    "        pup_date = data['published'][0]\n",
    "\n",
    "        # Update the log with publication date information\n",
    "        if pup_date in log.keys():\n",
    "            log[pup_date] += 1\n",
    "        else:\n",
    "            log[pup_date] = 1\n",
    "\n",
    "        # Create a folder for each publication date\n",
    "        download_folder = download_path / pup_date \n",
    "        Path(str(download_folder)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Update the log with a combination of publication date and category\n",
    "        if pup_date + ' ' + comp_cat in log.keys():\n",
    "            log[pup_date + ' ' + comp_cat] += 1\n",
    "        else:\n",
    "            log[pup_date + ' ' + comp_cat] = 1\n",
    "\n",
    "        # Update the log with category information\n",
    "        if comp_cat in log.keys():\n",
    "            log[comp_cat] += 1\n",
    "        else:\n",
    "            log[comp_cat] = 1\n",
    "\n",
    "        # Create a folder for each category\n",
    "        download_folder = download_folder / comp_cat \n",
    "        Path(str(download_folder)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Check if the entry has been downloaded before\n",
    "        is_downloaded = check_col_downloaded_value(df, comp_cat)\n",
    "\n",
    "        # If the entry has a PDF link and has not been downloaded, download it\n",
    "        if 'link' in data and 'pdf' in data['link'][0]:\n",
    "            download_link = data['link'][0]['pdf']\n",
    "            if title not in df['file_name'].tolist() or not is_downloaded:\n",
    "                downloader(download_link, download_folder, title)\n",
    "                downloaded = True\n",
    "\n",
    "        # If the entry has not been recorded in the DataFrame, record it\n",
    "        if title not in df['file_name'].tolist() or not is_downloaded:\n",
    "            new_row = {'file_name': title, 'path': download_folder, 'tag': comp_cat, 'downloaded': downloaded}\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "            # Save the updated DataFrame to the CSV file\n",
    "            df.to_csv('downloaded.csv', index=False)\n",
    "\n",
    "# Call the main function with specified arguments\n",
    "main(url, full_path, max=339)\n",
    "\n",
    "# Create a DataFrame from the log dictionary and save it to a CSV file\n",
    "log_df = pd.DataFrame([dict(sorted(log.items()))]).T\n",
    "log_df.to_csv('log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
